name: Rust

on:
  push:
    branches:
      - '**'
    tags:
      - 'v[0-9]+.[0-9]+.[0-9]+'
      - 'v[0-9]+.[0-9]+.[0-9]+-*'
  pull_request:

env:
  CARGO_TERM_COLOR: always

jobs:
  # ==========================================================================
  # Format Check
  # ==========================================================================
  format:
    name: Format
    runs-on: ubuntu-latest
    env:
      RUSTV: nightly-2024-04-09
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
      - name: Install
        run: |
          rustup toolchain install $RUSTV
          rustup component add --toolchain $RUSTV rustfmt
      - name: Check
        run: cargo +$RUSTV fmt -- --check

  # ==========================================================================
  # Phase 1: Build & Test with Coverage on ARM Runner
  # ==========================================================================
  test:
    name: Test
    runs-on: ubuntu-22.04-arm
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

      - name: Install Rust toolchain
        run: |
          rustup toolchain install stable --profile minimal
          rustup component add llvm-tools-preview

      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@30eab0fabba9ea3f522099957e668b21876aa39e # v2.66.6
        with:
          tool: cargo-llvm-cov

      - name: Run tests with coverage
        run: |
          cargo llvm-cov test --all-features --workspace \
            --lcov --output-path lcov.info

      - name: Generate coverage summary
        id: coverage
        run: |
          # Extract coverage percentage from lcov (handle empty results gracefully)
          TOTAL_LINES=$(grep -E "^LF:" lcov.info 2>/dev/null | cut -d: -f2 | paste -sd+ | bc 2>/dev/null || echo "0")
          COVERED_LINES=$(grep -E "^LH:" lcov.info 2>/dev/null | cut -d: -f2 | paste -sd+ | bc 2>/dev/null || echo "0")
          if [ -z "$TOTAL_LINES" ]; then TOTAL_LINES=0; fi
          if [ -z "$COVERED_LINES" ]; then COVERED_LINES=0; fi
          if [ "$TOTAL_LINES" -gt 0 ]; then
            COVERAGE=$(echo "scale=2; $COVERED_LINES * 100 / $TOTAL_LINES" | bc)
          else
            COVERAGE="0"
          fi
          echo "coverage=$COVERAGE" >> $GITHUB_OUTPUT
          echo "total_lines=$TOTAL_LINES" >> $GITHUB_OUTPUT
          echo "covered_lines=$COVERED_LINES" >> $GITHUB_OUTPUT

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@0561704f0f02c16a585d4c7555e57fa2e44cf909 # v5.5.2
        with:
          files: lcov.info
          flags: rust
          fail_ci_if_error: false

      - name: Write job summary
        run: |
          echo "## ðŸ§ª Rust Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Coverage** | ${{ steps.coverage.outputs.coverage }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| **Lines Covered** | ${{ steps.coverage.outputs.covered_lines }} / ${{ steps.coverage.outputs.total_lines }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Runner** | ubuntu-22.04-arm |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… All tests passed" >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage artifact
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: rust-coverage
          path: lcov.info
          retention-days: 30

  # ==========================================================================
  # Phase 1: Build Benchmark Binaries on ARM Runner
  # ==========================================================================
  build-benchmarks:
    name: Build Benchmarks
    runs-on: ubuntu-22.04-arm
    if: github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

      - name: Install Rust toolchain
        run: |
          rustup toolchain install stable --profile minimal

      - name: Build benchmark binaries
        run: |
          # Build all benchmark binaries in release mode
          cargo bench --no-run

          # Find and copy benchmark binaries (using -print0/xargs for robustness)
          mkdir -p benchmark-binaries
          find target/release/deps -maxdepth 1 -type f -executable -name "serialization*" \
            ! -name '*.so' ! -name '*.a' ! -name '*.d' ! -name '*.rlib' -print0 | \
            xargs -0 -I{} sh -c 'cp "$1" benchmark-binaries/ && echo "Copied: $(basename "$1")"' sh '{}'

          ls -la benchmark-binaries/

      - name: Upload benchmark binaries
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: benchmark-binaries-aarch64
          path: benchmark-binaries/
          retention-days: 7

  # ==========================================================================
  # Phase 2: Run Benchmarks on NXP i.MX 8M Plus Hardware
  # ==========================================================================
  run-benchmarks:
    name: Run Benchmarks
    needs: build-benchmarks
    runs-on: nxp-imx8mp-latest
    steps:
      - name: Clean workspace
        run: |
          rm -rf benchmark-binaries/ benchmark-results/ 2>/dev/null || true

      - name: Download benchmark binaries
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
        with:
          name: benchmark-binaries-aarch64
          path: benchmark-binaries/

      - name: Make binaries executable
        run: chmod +x benchmark-binaries/*

      - name: Run benchmarks
        run: |
          mkdir -p benchmark-results

          # Run each benchmark binary with Criterion options
          for bench_bin in benchmark-binaries/*; do
            if [ -x "$bench_bin" ]; then
              BENCH_NAME=$(basename "$bench_bin")
              echo "=== Running benchmark: $BENCH_NAME ==="

              # Run benchmark with JSON output for machine parsing
              # --noplot to skip HTML generation (no display on target)
              # --save-baseline to save results
              "$bench_bin" --bench --noplot \
                --save-baseline nxp-imx8mp \
                2>&1 | tee "benchmark-results/${BENCH_NAME}.txt"
            fi
          done

          echo "=== Benchmark files generated ==="
          ls -la benchmark-results/

      - name: Upload benchmark results
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: benchmark-results-raw
          path: benchmark-results/
          retention-days: 30

  # ==========================================================================
  # Phase 3: Process Benchmark Results and Generate Report
  # ==========================================================================
  process-benchmarks:
    name: Process Benchmark Results
    needs: run-benchmarks
    runs-on: ubuntu-22.04-arm
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

      - name: Download benchmark results
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
        with:
          name: benchmark-results-raw
          path: benchmark-results/

      - name: Parse and format benchmark results
        id: bench
        run: |
          # Combine all benchmark output files
          cat benchmark-results/*.txt > benchmark-output.txt

          # Parse Criterion output and generate JSON for processing
          # Criterion format:
          #   benchmark_name
          #                   time:   [lower median upper]
          #                   thrpt:  [lower median upper]  (optional)
          python3 << 'PARSE_SCRIPT'
          import re
          import json
          import sys

          benchmarks = []
          current_name = None

          with open('benchmark-output.txt', 'r') as f:
              lines = f.readlines()

          i = 0
          while i < len(lines):
              line = lines[i].rstrip()

              # Look for benchmark name lines (not indented, contains /)
              if line and not line.startswith(' ') and '/' in line and 'Benchmarking' not in line:
                  current_name = line.strip()

              # Look for time: line (must be primary, not "change" which has % values)
              elif 'time:' in line and current_name:
                  # Skip "change" lines which contain percentages
                  if '%' not in line:
                      # Extract median (middle value in brackets)
                      match = re.search(r'\[([\d.]+)\s*(\w+)\s+([\d.]+)\s*(\w+)\s+([\d.]+)\s*(\w+)\]', line)
                      if match:
                          median_val = match.group(3)
                          median_unit = match.group(4)
                          time_str = f"{median_val} {median_unit}"

                          # Look for throughput on next line
                          thrpt_str = "N/A"
                          if i + 1 < len(lines):
                              next_line = lines[i + 1]
                              if 'thrpt:' in next_line and '%' not in next_line:
                                  thrpt_match = re.search(r'\[([\d.]+)\s*(\S+)\s+([\d.]+)\s*(\S+)\s+([\d.]+)\s*(\S+)\]', next_line)
                                  if thrpt_match:
                                      thrpt_str = f"{thrpt_match.group(3)} {thrpt_match.group(4)}"

                          benchmarks.append({
                              'name': current_name,
                              'time': time_str,
                              'throughput': thrpt_str
                          })
                          current_name = None  # Reset after capturing
              i += 1

          # Write JSON for further processing
          with open('benchmarks.json', 'w') as f:
              json.dump(benchmarks, f, indent=2)

          print(f"Parsed {len(benchmarks)} benchmarks")
          PARSE_SCRIPT

          # Generate markdown summary from JSON
          python3 << 'GENERATE_SCRIPT'
          import json

          with open('benchmarks.json', 'r') as f:
              benchmarks = json.load(f)

          # Categorize benchmarks
          basic_msgs = []  # builtin_interfaces, std_msgs, geometry_msgs
          heavy_msgs = []  # Image, PointCloud2, Mask, RadarCube, CompressedVideo

          for b in benchmarks:
              name = b['name']
              if any(cat in name for cat in ['builtin_interfaces', 'std_msgs', 'geometry_msgs']):
                  basic_msgs.append(b)
              else:
                  heavy_msgs.append(b)

          with open('benchmark-summary.md', 'w') as f:
              f.write("## ðŸ“Š On-Target Benchmark Results\n\n")
              f.write("**Target Hardware:** NXP i.MX 8M Plus (Cortex-A53 @ 1.8GHz)\n")
              f.write("**Architecture:** aarch64\n")
              f.write(f"**Total Benchmarks:** {len(benchmarks)}\n\n")

              # Basic message types table
              f.write("### Basic Message Types\n\n")
              f.write("| Message | Serialize | Deserialize | Throughput (ser) |\n")
              f.write("|---------|-----------|-------------|------------------|\n")

              # Group by message type
              msg_data = {}
              for b in basic_msgs:
                  parts = b['name'].split('/')
                  if len(parts) >= 3:
                      msg_type = f"{parts[0]}/{parts[1]}"
                      op = parts[2]
                      if msg_type not in msg_data:
                          msg_data[msg_type] = {'serialize': 'N/A', 'deserialize': 'N/A', 'throughput': 'N/A'}
                      msg_data[msg_type][op] = b['time']
                      if op == 'serialize' and b['throughput'] != 'N/A':
                          msg_data[msg_type]['throughput'] = b['throughput']

              for msg_type in sorted(msg_data.keys()):
                  data = msg_data[msg_type]
                  f.write(f"| {msg_type} | {data['serialize']} | {data['deserialize']} | {data['throughput']} |\n")

              f.write("\n")

              # Throughput chart using Mermaid xychart
              f.write("### Serialization Throughput (Basic Types)\n\n")
              f.write("```mermaid\n")
              f.write("xychart-beta horizontal\n")
              f.write("    title \"Throughput (MiB/s) - Higher is Better\"\n")

              # Collect data for chart (only basic types with throughput)
              chart_data = []
              for msg_type in sorted(msg_data.keys()):
                  thrpt = msg_data[msg_type]['throughput']
                  if thrpt != 'N/A':
                      # Extract numeric value
                      val = float(thrpt.split()[0])
                      # Short label
                      label = msg_type.split('/')[-1]
                      chart_data.append((label, val))

              if chart_data:
                  labels = [f'"{d[0]}"' for d in chart_data]
                  values = [str(d[1]) for d in chart_data]
                  f.write(f"    x-axis [{', '.join(labels)}]\n")
                  f.write(f"    bar [{', '.join(values)}]\n")
              else:
                  f.write('    x-axis ["No data"]\n    bar [0]\n')

              f.write("```\n\n")

              # Heavy message performance by category
              heavy_categories = {
                  'Image': [],
                  'PointCloud2': [],
                  'Mask': [],
                  'RadarCube': [],
                  'FoxgloveCompressedVideo': []
              }

              for b in heavy_msgs:
                  for cat in heavy_categories:
                      if b['name'].startswith(cat + '/'):
                          heavy_categories[cat].append(b)
                          break

              for cat_name, cat_benchmarks in heavy_categories.items():
                  if not cat_benchmarks:
                      continue

                  f.write(f"### {cat_name} Performance\n\n")
                  f.write("| Variant | Serialize | Deserialize | Throughput |\n")
                  f.write("|---------|-----------|-------------|------------|\n")

                  # Group by variant - format is Category/operation/variant
                  variants = {}
                  for b in cat_benchmarks:
                      parts = b['name'].split('/')
                      # parts[0] = category, parts[1] = operation, parts[2] = variant
                      if len(parts) >= 3:
                          op = parts[1]  # serialize or deserialize
                          variant = parts[2]
                      else:
                          op = parts[1] if len(parts) > 1 else 'unknown'
                          variant = 'default'
                      if variant not in variants:
                          variants[variant] = {'serialize': 'N/A', 'deserialize': 'N/A', 'throughput': 'N/A'}
                      variants[variant][op] = b['time']
                      if op == 'serialize' and b['throughput'] != 'N/A':
                          variants[variant]['throughput'] = b['throughput']

                  for variant, data in variants.items():
                      f.write(f"| {variant} | {data['serialize']} | {data['deserialize']} | {data['throughput']} |\n")

                  f.write("\n")

              # Summary statistics
              f.write("### Summary\n\n")
              f.write(f"- **Basic message types:** {len(basic_msgs)} benchmarks\n")
              f.write(f"- **Heavy message types:** {len(heavy_msgs)} benchmarks\n")
              f.write(f"- **Total:** {len(benchmarks)} benchmarks\n")

          print("Generated benchmark-summary.md")
          GENERATE_SCRIPT

          # Count benchmarks for output
          BENCH_COUNT=$(grep -c "Benchmarking" benchmark-output.txt 2>/dev/null || echo "0")
          echo "bench_count=$BENCH_COUNT" >> $GITHUB_OUTPUT

      - name: Write job summary
        run: |
          cat benchmark-summary.md >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>ðŸ“‹ Full Benchmark Output</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat benchmark-output.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Upload processed results
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: benchmark-results
          path: |
            benchmark-output.txt
            benchmark-summary.md
          retention-days: 30

  # ==========================================================================
  # Deploy to crates.io
  # ==========================================================================
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    needs: [test, format]
    if: github.event_name == 'push' && contains(github.ref, 'refs/tags/')
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
      - uses: taiki-e/install-action@30eab0fabba9ea3f522099957e668b21876aa39e # v2.66.6
        with:
          tool: cargo-workspaces
      - name: Publish
        env:
          CARGO_REGISTRY_TOKEN: ${{ secrets.CARGO_REGISTRY_TOKEN }}
        run: cargo workspaces publish --from-git