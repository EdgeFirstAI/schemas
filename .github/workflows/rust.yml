name: Rust

on:
  push:
    branches:
      - '**'
    tags:
      - 'v[0-9]+.[0-9]+.[0-9]+'
      - 'v[0-9]+.[0-9]+.[0-9]+-*'
  pull_request:

env:
  CARGO_TERM_COLOR: always

jobs:
  # ==========================================================================
  # Format Check
  # ==========================================================================
  format:
    name: Format
    runs-on: ubuntu-latest
    env:
      RUSTV: nightly-2024-04-09
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
      - name: Install
        run: |
          rustup toolchain install $RUSTV
          rustup component add --toolchain $RUSTV rustfmt
      - name: Check
        run: cargo +$RUSTV fmt -- --check

  # ==========================================================================
  # Phase 1: Build & Test with Coverage on ARM Runner
  # ==========================================================================
  test:
    name: Test
    runs-on: ubuntu-22.04-arm
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

      - name: Install Rust toolchain
        run: |
          rustup toolchain install stable --profile minimal
          rustup component add llvm-tools-preview

      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@30eab0fabba9ea3f522099957e668b21876aa39e # v2.66.6
        with:
          tool: cargo-llvm-cov

      - name: Run tests with coverage
        run: |
          cargo llvm-cov test --all-features --workspace \
            --lcov --output-path lcov.info

      - name: Generate coverage summary
        id: coverage
        run: |
          # Extract coverage percentage from lcov (handle empty results gracefully)
          TOTAL_LINES=$(grep -E "^LF:" lcov.info 2>/dev/null | cut -d: -f2 | paste -sd+ | bc 2>/dev/null || echo "0")
          COVERED_LINES=$(grep -E "^LH:" lcov.info 2>/dev/null | cut -d: -f2 | paste -sd+ | bc 2>/dev/null || echo "0")
          if [ -z "$TOTAL_LINES" ]; then TOTAL_LINES=0; fi
          if [ -z "$COVERED_LINES" ]; then COVERED_LINES=0; fi
          if [ "$TOTAL_LINES" -gt 0 ]; then
            COVERAGE=$(echo "scale=2; $COVERED_LINES * 100 / $TOTAL_LINES" | bc)
          else
            COVERAGE="0"
          fi
          echo "coverage=$COVERAGE" >> $GITHUB_OUTPUT
          echo "total_lines=$TOTAL_LINES" >> $GITHUB_OUTPUT
          echo "covered_lines=$COVERED_LINES" >> $GITHUB_OUTPUT

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@0561704f0f02c16a585d4c7555e57fa2e44cf909 # v5.5.2
        with:
          files: lcov.info
          flags: rust
          fail_ci_if_error: false

      - name: Write job summary
        run: |
          echo "## ðŸ§ª Rust Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Coverage** | ${{ steps.coverage.outputs.coverage }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| **Lines Covered** | ${{ steps.coverage.outputs.covered_lines }} / ${{ steps.coverage.outputs.total_lines }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Runner** | ubuntu-22.04-arm |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… All tests passed" >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage artifact
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: rust-coverage
          path: lcov.info
          retention-days: 30

  # ==========================================================================
  # Phase 1: Build Benchmark Binaries on ARM Runner
  # ==========================================================================
  build-benchmarks:
    name: Build Benchmarks
    runs-on: ubuntu-22.04-arm
    if: github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

      - name: Install Rust toolchain
        run: |
          rustup toolchain install stable --profile minimal

      - name: Build benchmark binaries
        run: |
          # Build all benchmark binaries in release mode
          cargo bench --no-run

          # Find and copy benchmark binaries (using -print0/xargs for robustness)
          mkdir -p benchmark-binaries
          find target/release/deps -maxdepth 1 -type f -executable -name "serialization*" \
            ! -name '*.so' ! -name '*.a' ! -name '*.d' ! -name '*.rlib' -print0 | \
            xargs -0 -I{} sh -c 'cp "$1" benchmark-binaries/ && echo "Copied: $(basename "$1")"' sh '{}'

          ls -la benchmark-binaries/

      - name: Upload benchmark binaries
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: benchmark-binaries-aarch64
          path: benchmark-binaries/
          retention-days: 7

  # ==========================================================================
  # Phase 2: Run Benchmarks on NXP i.MX 8M Plus Hardware
  # ==========================================================================
  run-benchmarks:
    name: Run Benchmarks
    needs: build-benchmarks
    runs-on: nxp-imx8mp-latest
    steps:
      - name: Clean workspace
        run: |
          rm -rf benchmark-binaries/ benchmark-results/ 2>/dev/null || true

      - name: Download benchmark binaries
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
        with:
          name: benchmark-binaries-aarch64
          path: benchmark-binaries/

      - name: Make binaries executable
        run: chmod +x benchmark-binaries/*

      - name: Run benchmarks
        run: |
          mkdir -p benchmark-results

          # Run each benchmark binary with Criterion options
          for bench_bin in benchmark-binaries/*; do
            if [ -x "$bench_bin" ]; then
              BENCH_NAME=$(basename "$bench_bin")
              echo "=== Running benchmark: $BENCH_NAME ==="

              # Run benchmark with JSON output for machine parsing
              # --noplot to skip HTML generation (no display on target)
              # --save-baseline to save results
              "$bench_bin" --bench --noplot \
                --save-baseline nxp-imx8mp \
                2>&1 | tee "benchmark-results/${BENCH_NAME}.txt"
            fi
          done

          echo "=== Benchmark files generated ==="
          ls -la benchmark-results/

      - name: Upload benchmark results
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: benchmark-results-raw
          path: benchmark-results/
          retention-days: 30

  # ==========================================================================
  # Phase 3: Process Benchmark Results and Generate Report
  # ==========================================================================
  process-benchmarks:
    name: Process Benchmark Results
    needs: run-benchmarks
    runs-on: ubuntu-22.04-arm
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

      - name: Download benchmark results
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
        with:
          name: benchmark-results-raw
          path: benchmark-results/

      - name: Parse and format benchmark results
        id: bench
        run: |
          # Combine all benchmark output files
          cat benchmark-results/*.txt > benchmark-output.txt

          # Parse Criterion output and generate JSON for processing
          # Criterion format:
          #   benchmark_name
          #                   time:   [lower median upper]
          #                   thrpt:  [lower median upper]  (optional)
          python3 << 'PARSE_SCRIPT'
          import re
          import json
          import sys

          benchmarks = []
          current_name = None

          with open('benchmark-output.txt', 'r') as f:
              lines = f.readlines()

          i = 0
          while i < len(lines):
              line = lines[i].rstrip()

              # Look for benchmark name lines (not indented, contains /)
              if line and not line.startswith(' ') and '/' in line and 'Benchmarking' not in line:
                  current_name = line.strip()

              # Look for time: line (must be primary, not "change" which has % values)
              elif 'time:' in line and current_name:
                  # Skip "change" lines which contain percentages
                  if '%' not in line:
                      # Extract median (middle value in brackets)
                      match = re.search(r'\[([\d.]+)\s*(\w+)\s+([\d.]+)\s*(\w+)\s+([\d.]+)\s*(\w+)\]', line)
                      if match:
                          median_val = match.group(3)
                          median_unit = match.group(4)
                          time_str = f"{median_val} {median_unit}"

                          # Look for throughput on next line
                          thrpt_str = "N/A"
                          if i + 1 < len(lines):
                              next_line = lines[i + 1]
                              if 'thrpt:' in next_line and '%' not in next_line:
                                  thrpt_match = re.search(r'\[([\d.]+)\s*(\S+)\s+([\d.]+)\s*(\S+)\s+([\d.]+)\s*(\S+)\]', next_line)
                                  if thrpt_match:
                                      thrpt_str = f"{thrpt_match.group(3)} {thrpt_match.group(4)}"

                          benchmarks.append({
                              'name': current_name,
                              'time': time_str,
                              'throughput': thrpt_str
                          })
                          current_name = None  # Reset after capturing
              i += 1

          # Write JSON for further processing
          with open('benchmarks.json', 'w') as f:
              json.dump(benchmarks, f, indent=2)

          print(f"Parsed {len(benchmarks)} benchmarks")
          PARSE_SCRIPT

          # Generate markdown summary from JSON
          python3 << 'GENERATE_SCRIPT'
          import json
          import re

          with open('benchmarks.json', 'r') as f:
              benchmarks = json.load(f)

          # Size mapping for variants (approximate payload sizes in bytes)
          SIZE_MAP = {
              # Image variants
              'VGA_rgb8': 921600,           # 640x480x3
              'HD_rgb8': 2764800,           # 1280x720x3
              'FHD_rgb8': 6220800,          # 1920x1080x3
              'FHD_yuyv': 4147200,          # 1920x1080x2
              # PointCloud2 variants
              'sparse_1K': 16000,           # 1K points * 16 bytes
              'medium_10K': 160000,         # 10K points * 16 bytes
              'dense_65K': 1048576,         # 65K points * 16 bytes
              'very_dense_131K': 2097152,   # 131K points * 16 bytes
              # Mask variants
              'small_256x256': 65536,       # 256x256
              'medium_640x480': 307200,     # 640x480
              'large_1920x1080': 2073600,   # 1920x1080
              'multiclass_640x480x20': 6144000,  # 640x480x20
              # RadarCube variants
              'small_128KB': 131072,
              'medium_2MB': 2097152,
              'large_24MB': 25165824,
              'small_complex': 131072,
              # CompressedVideo variants
              '10KB': 10240,
              '100KB': 102400,
              '500KB': 512000,
              '1MB': 1048576,
          }

          def parse_time_to_us(time_str):
              """Convert time string to microseconds for comparison."""
              if time_str == 'N/A':
                  return None
              match = re.match(r'([\d.]+)\s*(\w+)', time_str)
              if not match:
                  return None
              val, unit = float(match.group(1)), match.group(2)
              multipliers = {'ns': 0.001, 'Âµs': 1, 'us': 1, 'ms': 1000, 's': 1000000}
              return val * multipliers.get(unit, 1)

          def format_size(bytes_val):
              """Format byte size for display."""
              if bytes_val >= 1048576:
                  return f"{bytes_val / 1048576:.1f} MB"
              elif bytes_val >= 1024:
                  return f"{bytes_val / 1024:.0f} KB"
              return f"{bytes_val} B"

          # Categorize benchmarks
          basic_msgs = []
          heavy_msgs = []

          for b in benchmarks:
              name = b['name']
              if any(cat in name for cat in ['builtin_interfaces', 'std_msgs', 'geometry_msgs']):
                  basic_msgs.append(b)
              else:
                  heavy_msgs.append(b)

          # Heavy message categories with descriptions
          heavy_categories = {
              'Image': {'benchmarks': [], 'desc': 'Camera frame serialization'},
              'PointCloud2': {'benchmarks': [], 'desc': 'LiDAR point cloud data'},
              'Mask': {'benchmarks': [], 'desc': 'Segmentation mask data'},
              'RadarCube': {'benchmarks': [], 'desc': 'Radar cube tensors'},
              'FoxgloveCompressedVideo': {'benchmarks': [], 'desc': 'Compressed video frames'},
          }

          for b in heavy_msgs:
              for cat in heavy_categories:
                  if b['name'].startswith(cat + '/'):
                      heavy_categories[cat]['benchmarks'].append(b)
                      break

          with open('benchmark-summary.md', 'w') as f:
              f.write("## ðŸ“Š On-Target Benchmark Results\n\n")
              f.write("**Target Hardware:** NXP i.MX 8M Plus (Cortex-A53 @ 1.8GHz)\n")
              f.write("**Architecture:** aarch64\n")
              f.write(f"**Total Benchmarks:** {len(benchmarks)}\n\n")

              # Collect all heavy message data for the chart
              all_chart_data = []  # [(category, variant, size_bytes, serialize_us, deserialize_us)]

              for cat_name, cat_info in heavy_categories.items():
                  cat_benchmarks = cat_info['benchmarks']
                  if not cat_benchmarks:
                      continue

                  f.write(f"### {cat_name}\n\n")
                  f.write(f"*{cat_info['desc']}*\n\n")
                  f.write("| Variant | Size | Serialize | Deserialize |\n")
                  f.write("|---------|------|-----------|-------------|\n")

                  # Group by variant
                  variants = {}
                  for b in cat_benchmarks:
                      parts = b['name'].split('/')
                      if len(parts) >= 3:
                          op = parts[1]
                          variant = parts[2]
                      else:
                          op = parts[1] if len(parts) > 1 else 'unknown'
                          variant = 'default'
                      if variant not in variants:
                          variants[variant] = {'serialize': 'N/A', 'deserialize': 'N/A'}
                      variants[variant][op] = b['time']

                  # Sort variants by size
                  sorted_variants = sorted(variants.items(),
                      key=lambda x: SIZE_MAP.get(x[0], 0))

                  for variant, data in sorted_variants:
                      size_bytes = SIZE_MAP.get(variant, 0)
                      size_str = format_size(size_bytes) if size_bytes > 0 else '?'
                      f.write(f"| {variant} | {size_str} | {data['serialize']} | {data['deserialize']} |\n")

                      # Collect for chart
                      ser_us = parse_time_to_us(data['serialize'])
                      deser_us = parse_time_to_us(data['deserialize'])
                      if ser_us is not None and size_bytes > 0:
                          all_chart_data.append((cat_name, variant, size_bytes, ser_us, deser_us))

                  f.write("\n")

              # Latency vs Size Chart using Mermaid xychart
              f.write("### Serialization Latency vs Message Size\n\n")
              f.write("```mermaid\n")
              f.write("xychart-beta\n")
              f.write("    title \"Serialize Latency (ms) by Message Size\"\n")
              f.write("    x-axis \"Message Size\"\n")
              f.write("    y-axis \"Time (ms)\" 0 --> 120\n")

              # Sort by size and create chart data
              sorted_chart = sorted(all_chart_data, key=lambda x: x[2])
              if sorted_chart:
                  # Use variant names as x-axis labels, time in ms as values
                  labels = [f'"{d[1]}"' for d in sorted_chart]
                  ser_values = [f"{d[3]/1000:.2f}" for d in sorted_chart]  # Convert Âµs to ms
                  f.write(f"    x-axis [{', '.join(labels)}]\n")
                  f.write(f"    line [{', '.join(ser_values)}]\n")

              f.write("```\n\n")

              # Deserialization chart
              f.write("### Deserialization Latency vs Message Size\n\n")
              f.write("```mermaid\n")
              f.write("xychart-beta\n")
              f.write("    title \"Deserialize Latency (ms) by Message Size\"\n")
              f.write("    x-axis \"Message Size\"\n")
              f.write("    y-axis \"Time (ms)\" 0 --> 120\n")

              if sorted_chart:
                  labels = [f'"{d[1]}"' for d in sorted_chart]
                  deser_values = [f"{(d[4] or 0)/1000:.2f}" for d in sorted_chart]
                  f.write(f"    x-axis [{', '.join(labels)}]\n")
                  f.write(f"    line [{', '.join(deser_values)}]\n")

              f.write("```\n\n")

              # Basic message types summary (collapsed)
              f.write("<details>\n")
              f.write("<summary>Basic Message Types (click to expand)</summary>\n\n")
              f.write("| Message | Serialize | Deserialize |\n")
              f.write("|---------|-----------|-------------|\n")

              msg_data = {}
              for b in basic_msgs:
                  parts = b['name'].split('/')
                  if len(parts) >= 3:
                      msg_type = f"{parts[0]}/{parts[1]}"
                      op = parts[2]
                      if msg_type not in msg_data:
                          msg_data[msg_type] = {'serialize': 'N/A', 'deserialize': 'N/A'}
                      msg_data[msg_type][op] = b['time']

              for msg_type in sorted(msg_data.keys()):
                  data = msg_data[msg_type]
                  f.write(f"| {msg_type} | {data['serialize']} | {data['deserialize']} |\n")

              f.write("\n</details>\n\n")

              # Summary statistics
              f.write("### Summary\n\n")
              f.write(f"- **Heavy message benchmarks:** {len(heavy_msgs)}\n")
              f.write(f"- **Basic message benchmarks:** {len(basic_msgs)}\n")
              f.write(f"- **Total:** {len(benchmarks)}\n")

          print("Generated benchmark-summary.md")
          GENERATE_SCRIPT

          # Count benchmarks for output
          BENCH_COUNT=$(grep -c "Benchmarking" benchmark-output.txt 2>/dev/null || echo "0")
          echo "bench_count=$BENCH_COUNT" >> $GITHUB_OUTPUT

      - name: Write job summary
        run: |
          cat benchmark-summary.md >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>ðŸ“‹ Full Benchmark Output</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat benchmark-output.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Upload processed results
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: benchmark-results
          path: |
            benchmark-output.txt
            benchmark-summary.md
          retention-days: 30

  # ==========================================================================
  # Deploy to crates.io
  # ==========================================================================
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    needs: [test, format]
    if: github.event_name == 'push' && contains(github.ref, 'refs/tags/')
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
      - uses: taiki-e/install-action@30eab0fabba9ea3f522099957e668b21876aa39e # v2.66.6
        with:
          tool: cargo-workspaces
      - name: Publish
        env:
          CARGO_REGISTRY_TOKEN: ${{ secrets.CARGO_REGISTRY_TOKEN }}
        run: cargo workspaces publish --from-git